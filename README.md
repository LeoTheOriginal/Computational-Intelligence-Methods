# Rule Compression for Donor Biomarker Data

## Overview

This repository contains a solution for compressing a set of association rules generated from donor biomarker data. The objective of the project is to reduce redundancy in the rule set while preserving the key insights regarding which biomarkers indicate an "old" donor.

The solution leverages two input files:
- **dataset.tsv**: A tab-separated file with donor data, where each biomarker is represented as a logical predicate (with boolean values). The dataset may contain missing values.
- **rules.txt**: A file containing the generated rules in the format `LHS => donor_is_old`. The LHS (left-hand side) is a conjunction (using the `AND` keyword) of predicates, where each predicate may be negated using the keyword `NOT`.

## Approach and Heuristics

Our solution is implemented in Python (and can be run in Google Colab or any Jupyter environment) and follows these steps:

1. **Data Loading:**  
   Both the dataset and rules are downloaded and loaded. The dataset is processed so that text representations of booleans (`TRUE`/`FALSE`) are converted to proper boolean types.

2. **Rule Evaluation and Metrics Calculation:**  
   For each rule from `rules.txt`, we:
   - Parse the rule into its constituent conditions.
   - Evaluate the rule on the dataset to compute its **support** (the fraction of records where the LHS holds true) and **confidence** (the fraction of those records that also have `donor_is_old` true).
   - Calculate a simple **score** as `support * confidence`, which serves as a measure of the rule's usefulness.

3. **Filtering and Compression:**  
   - **Filtering:** We remove rules that do not appear in the data (i.e., rules with zero support).
   - **Redundancy Elimination:** We compare rules to identify if the conditions of one rule are a subset of another. If a more general rule (with fewer conditions) achieves an equal or better score than a more specific one, the specific rule is considered redundant and removed.
   
4. **Output:**  
   The resulting compressed set of rules is saved to `compressed_rules.txt`.

## Files in the Repository

- **README.md**: This file.
- **JetBrains_Researcher.ipynb**: The Google Colab notebook containing the complete solution code.
- **dataset.tsv**: The dataset file containing donor biomarker data (automatically downloaded in the notebook).
- **rules.txt**: The file with the generated rules (automatically downloaded in the notebook).
- **compressed_rules.txt**: The output file generated by the notebook that contains the compressed set of rules.

## How to Run

1. **Google Colab:**
   - Open the `colab_notebook.ipynb` notebook in [Google Colab](https://colab.research.google.com).
   - Run the notebook cells sequentially. The notebook will install necessary packages, download the input files from Google Drive, process the data, compute the rule metrics, compress the rules, and save the result to `compressed_rules.txt`.

2. **Local Environment:**
   - Ensure that Python 3 is installed along with the required packages (e.g., `pandas`, `gdown`).
   - Run the notebook (or convert the notebook to a Python script) to execute the entire pipeline.

## Dependencies

- Python 3.11.11
- [pandas](https://pandas.pydata.org/)
- [gdown](https://github.com/wkentaro/gdown) (for downloading files from Google Drive)

## Future Improvements

- **Advanced Scoring:** Incorporate additional metrics like lift to better assess rule usefulness.
- **Rule Merging:** Develop heuristics to merge similar rules that differ only by a minor condition, further reducing redundancy.
- **Dynamic Thresholds:** Introduce minimal thresholds for support and confidence to filter out statistically insignificant rules.

---

Feel free to reach out with any questions or suggestions for improvement.
